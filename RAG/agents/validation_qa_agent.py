import ollama
from pydantic import BaseModel, Field, ValidationError
from typing import Dict, Any, List
import json

class OllamaValidationResult(BaseModel):
    is_valid: bool = Field(..., description="True if the AI's answer is valid, faithful, and relevant; False otherwise.")
    reason: str = Field(..., description="A brief explanation if the answer is invalid, or confirmation if valid.")
    faithfulness_score: float = Field(0.0, ge=0.0, le=1.0, description="Score (0-1) indicating how well the answer is supported by the context.")
    relevance_score: float = Field(0.0, ge=0.0, le=1.0, description="Score (0-1) indicating how relevant the answer is to the user query.")

class ValidationQAAgent:
    """
    The Validation/QA Agent is responsible for evaluating the quality, accuracy,
    and faithfulness of the RAG Agent's generated response to the provided context
    using an Ollama Llama3.2 model.
    """
    def __init__(self):
        self.model_name = "llama3.2"
        self.temperature = 0.1

    async def _call_ollama_llama3_2_structured(self, prompt_message: str) -> OllamaValidationResult:
        """
        Makes an asynchronous call to the Ollama Llama3.2 model for structured validation.
        """
        try:
            response_structured = ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'user', 'content': prompt_message}
                ],
                format=OllamaValidationResult.model_json_schema(),
                options={'temperature': self.temperature},
            )

            if response_structured and 'message' in response_structured and 'content' in response_structured['message']:
                raw_json_content = response_structured['message']['content']

                try:
                    parsed_response = OllamaValidationResult.model_validate_json(raw_json_content)
                    return parsed_response
                except ValidationError as e:
                    return OllamaValidationResult(is_valid=False, reason=f"Model output format invalid: {e}")
                except json.JSONDecodeError as e:
                    return OllamaValidationResult(is_valid=False, reason=f"Model returned malformed JSON: {e}")
            else:
                return OllamaValidationResult(is_valid=False, reason="Empty or unexpected response from Ollama model.")

        except Exception as e:
            return OllamaValidationResult(is_valid=False, reason=f"Ollama API call failed: {e}")


    async def validate(self, rag_response: Dict[str, Any], user_query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Validates the RAG Agent's response against the user query and retrieved context
        using the Llama3.2 model.

        Args:
            rag_response (Dict[str, Any]): The response generated by the RAG Agent,
                                           containing 'response_text' and 'sources'.
            user_query (str): The original query from the user.
            retrieved_chunks (List[Dict[str, Any]]): The chunks used to generate the response.

        Returns:
            Dict[str, Any]: A dictionary indicating if the response is valid and a reason if not.
                            Example: {"is_valid": True, "reason": "Response is relevant and grounded."}
        """

        response_text = rag_response.get("response_text", "")
        if not response_text.strip():
            return {"is_valid": False, "reason": "Response is empty."}

        context_texts = [chunk["text"] for chunk in retrieved_chunks]
        context_str = "\n".join([f"Chunk {i+1}:\n{text}" for i, text in enumerate(context_texts)])
        if not context_str:
            context_str = "No context was provided for this query."

        prompt = (
            f"You are a highly analytical Quality Assurance agent for a RAG (Retrieval Augmented Generation) system. "
            f"Your task is to evaluate an AI's answer based on a user's query and the context provided for that answer. "
            f"Your judgment must be strictly based on the provided information.\n\n"
            f"Critically assess the AI's answer for the following:\n"
            f"1.  **Faithfulness/Groundedness**: Is every factual statement in the AI's answer directly supported by the provided context? (Avoids hallucinations).\n"
            f"2.  **Relevance**: Does the AI's answer directly address the user's query?\n"
            f"3.  **Completeness**: Does the AI's answer include all relevant information from the context that directly pertains to the query?\n"
            f"4.  **Conciseness**: Is the answer free from unnecessary fluff or irrelevant details?\n\n"
            f"If the AI's answer explicitly states it cannot answer based on the context (e.g., 'I don't have enough information'), "
            f"and there truly isn't enough information in the context to answer, then consider it valid for abstaining.\n\n"
            f"User Query: {user_query}\n\n"
            f"Provided Context:\n---\n{context_str}\n---\n\n"
            f"AI's Answer: {response_text}\n\n"
            f"Based on the above, determine if the AI's answer is 'valid'. Provide a 'reason' for your decision. "
            f"Your response MUST be a JSON object with 'is_valid' (boolean) and 'reason' (string) keys, "
            f"following the schema: {OllamaValidationResult.model_json_schema()}"
        )

        ollama_validation_result = await self._call_ollama_llama3_2_structured(prompt)
        return ollama_validation_result.model_dump()